la-k8s cluster

yum install net-tools wget mlocate bind-utils -y


master1

yum install -y docker
systemctl enable docker && systemctl start docker

cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
EOF

setenforce 0

yum install -y kubelet kubeadm kubectl
systemctl enable kubelet && systemctl start kubelet

cat <<EOF >  /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
sysctl --system

docker info | grep -i cgroup
cat /etc/systemd/system/kubelet.service.d/10-kubeadm.conf

systemctl daemon-reload
systemctl restart kubelet

swapoff -a
service kubelet stop
rm -rf /var/lib/kubelet/pki

kubeadm init --apiserver-advertise-address=192.168.33.21 --token=mydemo.0123456789abcdef --pod-network-cidr=10.244.0.0/16


(do we need to add token and pod-network-cidr to this)

kubeadm join --token d9201a.d7e651dd9902203d 10.0.2.15:6443 --discovery-token-ca-cert-hash sha256:e6ddc6acd7c03ed3ed3ba268ecf3f138f3a8cf3c7577c003d49e838eef9416d7

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config


kubeadm join --token mydemo.0123456789abcdef 192.168.33.21:6443 --discovery-token-ca-cert-hash sha256:381d7be2615f303530eb81887468e6e3fa4276089e7c9d928a7b07050aae0495

Install a POD network...

kubectl apply -f https://raw.githubusercontent.com/projectcalico/canal/master/k8s-install/1.7/rbac.yaml
kubectl apply -f https://raw.githubusercontent.com/projectcalico/canal/master/k8s-install/1.7/canal.yaml

WORKER NODES

yum install -y docker
systemctl enable docker && systemctl start docker
cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
EOF

setenforce 0

yum install -y kubelet kubeadm kubectl
systemctl enable kubelet && systemctl start kubelet

cat <<EOF >  /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
sysctl --system
systemctl daemon-reload
systemctl restart kubelet

swapoff -a
service kubelet stop
rm -rf /var/lib/kubelet/pki
kubeadm join --token mydemo.0123456789abcdef 192.168.33.21:6443 --discovery-token-unsafe-skip-ca-verification


CONFIGURE DASHBOARD

kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml
kubectl proxy


###############################

HA Masters...

On ALL MASTERS

curl -o /usr/bin/cfssl https://pkg.cfssl.org/R1.2/cfssl_linux-amd64
curl -o /usr/bin/cfssljson https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64
chmod +x /usr/bin/cfssl*

ON MASTER1

mkdir -p /etc/kubernetes/pki/etcd
cd /etc/kubernetes/pki/etcd

cat >ca-config.json <<EOF
{
    "signing": {
        "default": {
            "expiry": "43800h"
        },
        "profiles": {
            "server": {
                "expiry": "43800h",
                "usages": [
                    "signing",
                    "key encipherment",
                    "server auth",
                    "client auth"
                ]
            },
            "client": {
                "expiry": "43800h",
                "usages": [
                    "signing",
                    "key encipherment",
                    "client auth"
                ]
            },
            "peer": {
                "expiry": "43800h",
                "usages": [
                    "signing",
                    "key encipherment",
                    "server auth",
                    "client auth"
                ]
            }
        }
    }
}
EOF

cat >ca-csr.json <<EOF
{
    "CN": "etcd",
    "key": {
        "algo": "rsa",
        "size": 2048
    }
}
EOF

Use full paths or add /usr/local/bin to path

cfssl gencert -initca ca-csr.json | cfssljson -bare ca -

cat >client.json <<EOF
{
    "CN": "client",
    "key": {
        "algo": "ecdsa",
        "size": 256
    }
}
EOF

export PATH=$PATH:/usr/local/bin

cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=client client.json | cfssljson -bare client

export PEER_NAME=$(hostname)
export PRIVATE_IP=$(ip addr show eth1 | grep -Po 'inet \K[\d.]+')

generate ssh keys....

ssh-keygen -t rsa -b 4096 -C "" -q -N "" -f /root/.ssh/id_rsa

Public keys for etcd1 and 2 need to be in the authorized_keys file for etcd0

chmod 0640 /root/.ssh/authorized_keys

ON ETCD1 & 2 (e.g. la-k8s-master2 and 3)

mkdir -p /etc/kubernetes/pki/etcd
cd /etc/kubernetes/pki/etcd
scp root@la-k8s-master1:/etc/kubernetes/pki/etcd/ca.pem .
scp root@la-k8s-master1:/etc/kubernetes/pki/etcd/ca-key.pem .
scp root@la-k8s-master1:/etc/kubernetes/pki/etcd/client.pem .
scp root@la-k8s-master1:/etc/kubernetes/pki/etcd/client-key.pem .
scp root@la-k8s-master1:/etc/kubernetes/pki/etcd/ca-config.json .

RUN ON ALL ETCD MACHINES

export PEER_NAME=$(hostname)
export PRIVATE_IP=$(ip addr show eth1 | grep -Po 'inet \K[\d.]+')

cfssl print-defaults csr > config.json
sed -i '0,/CN/{s/example\.net/'"$PEER_NAME"'/}' config.json
sed -i 's/www\.example\.net/'"$PRIVATE_IP"'/' config.json
sed -i 's/example\.net/'"$PEER_NAME"'/' config.json

cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=server config.json | /usr/local/bin/cfssljson -bare server
cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=peer config.json | /usr/local/bin/cfssljson -bare peer

MIGHT NEED TO RE-RUN above as it didn't create files first time: peer.pem, peer-key.pem, server.pem, server-key.pem

SYSTEMD approach...

export ETCD_VERSION=v3.1.10
curl -sSL https://github.com/coreos/etcd/releases/download/${ETCD_VERSION}/etcd-${ETCD_VERSION}-linux-amd64.tar.gz | tar -xzv --strip-components=1 -C /usr/local/bin/
rm -rf etcd-$ETCD_VERSION-linux-amd64*

touch /etc/etcd.env
echo "PEER_NAME=$PEER_NAME" >> /etc/etcd.env
echo "PRIVATE_IP=$PRIVATE_IP" >> /etc/etcd.env

/etc/systemd/system/etcd.service should look like this...

[Unit]
Description=etcd
Documentation=https://github.com/coreos/etcd
Conflicts=etcd.service
Conflicts=etcd2.service

[Service]
EnvironmentFile=/etc/etcd.env
Type=notify
Restart=always
RestartSec=5s
LimitNOFILE=40000
TimeoutStartSec=0

ExecStart=/usr/local/bin/etcd --name=la-k8s-master1 --data-dir=/var/lib/etcd --listen-client-urls=https://192.168.33.21:2379 --advertise-client-urls=https://192.168.33.21:2379 --listen-peer-urls=https://192.168.33.21:2380 --initial-advertise-peer-urls=https://192.168.33.21:2380 --cert-file=/etc/kubernetes/pki/etcd/server.pem --key-file=/etc/kubernetes/pki/etcd/server-key.pem --client-cert-auth --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.pem --peer-cert-file=/etc/kubernetes/pki/etcd/peer.pem --peer-key-file=/etc/kubernetes/pki/etcd/peer-key.pem --peer-client-cert-auth --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.pem --initial-cluster la-k8s-master1=https://192.168.33.21:2380,la-k8s-master2=https://192.168.33.22:2380,la-k8s-master3=https://192.168.33.23:2380 --initial-cluster-token=my-etcd-token --initial-cluster-state=new

[Install]
WantedBy=multi-user.target



============================================= IGNORE THIS SECTION

cat >/etc/kubernetes/manifests/etcd.yaml <<EOF
apiVersion: v1
kind: Pod
metadata:
labels:
    component: etcd
    tier: control-plane
name: etcd0/1/2 *******************************CHANGE THIS
namespace: kube-system
spec:
containers:
- command:
    - etcd --name ${PEER_NAME} \
    - --data-dir /var/lib/etcd \
    - --listen-client-urls https://${PRIVATE_IP}:2379 \
    - --advertise-client-urls https://${PRIVATE_IP}:2379 \
    - --listen-peer-urls https://${PRIVATE_IP}:2380 \
    - --initial-advertise-peer-urls https://${PRIVATE_IP}:2380 \
    - --cert-file=/certs/server.pem \
    - --key-file=/certs/server-key.pem \
    - --client-cert-auth \
    - --trusted-ca-file=/certs/ca.pem \
    - --peer-cert-file=/certs/peer.pem \
    - --peer-key-file=/certs/peer-key.pem \
    - --peer-client-cert-auth \
    - --peer-trusted-ca-file=/certs/ca.pem \
    - --initial-cluster etcd0=https://192.168.33.21:2380,etcd1=https://192.168.33.22:2380,etcd2=https://192.168.33.23:2380 \
    - --initial-cluster-token my-etcd-token \
    - --initial-cluster-state new
    image: gcr.io/google_containers/etcd-amd64:3.1.0
    livenessProbe:
    httpGet:
        path: /health
        port: 2379
        scheme: HTTP
    initialDelaySeconds: 15
    timeoutSeconds: 15
    name: etcd
    env:
    - name: PUBLIC_IP
    valueFrom:
        fieldRef:
        fieldPath: status.hostIP
    - name: PRIVATE_IP
    valueFrom:
        fieldRef:
        fieldPath: status.podIP
    - name: PEER_NAME
    valueFrom:
        fieldRef:
        fieldPath: metadata.name
    volumeMounts:
    - mountPath: /var/lib/etcd
    name: etcd
    - mountPath: /certs
    name: certs
hostNetwork: true
volumes:
- hostPath:
    path: /var/lib/etcd
    type: DirectoryOrCreate
    name: etcd
- hostPath:
    path: /etc/kubernetes/pki/etcd
    name: certs
EOF

===============================================================================

Install keepalived...on all masters

yum install keepalived -y

Edit /etc/keepalived/keepalived.conf on first MASTER

! Configuration File for keepalived
global_defs {
   router_id LVS_DEVEL
}
    
vrrp_script check_apiserver {
   script "/etc/keepalived/check_apiserver.sh"
   interval 3
   weight -2
   fall 10
   rise 2
}
    
vrrp_instance VI_1 {
     state MASTER
     interface eth1
     virtual_router_id 51
     priority 101
     authentication {
         auth_type PASS
         auth_pass 4be37dc3b4c90194d1600c483e10ad1d
     }
     virtual_ipaddress {
         192.168.34.100
     }
     track_script {
         check_apiserver
     }
}

 ON OTHER MASTER NODES
! Configuration File for keepalived
global_defs {
   router_id LVS_DEVEL
}
    
vrrp_script check_apiserver {
   script "/etc/keepalived/check_apiserver.sh"
   interval 3
   weight -2
   fall 10
   rise 2
}
    
vrrp_instance VI_1 {
    state BACKUP
    interface eth1
    virtual_router_id 51
    priority 100
    authentication {
        auth_type PASS
        auth_pass 4be37dc3b4c90194d1600c483e10ad1d
    }
    virtual_ipaddress {
        192.168.33.100
    }
    track_script {
        check_apiserver
    }
}

 Edit /etc/keepalived/check_apiserver.sh

#!/bin/sh

errorExit() {
     echo "*** $*" 1>&2
     exit 1
}

curl --silent --max-time 2 --insecure https://localhost:6443/ -o /dev/null || errorExit "Error GET https://localhost:6443/"
if ip addr | grep -q 192.168.33.100; then
    curl --silent --max-time 2 --insecure https://192.168.33.100:6443/ -o /dev/null || errorExit "Error GET https://192.168.33.100:6443/"
fi

chmod +x /etc/keepalived/check_apiserver.sh
systemctl enable keepalived && systemctl start keepalived


KUBEADM INIT ON MASTER0

Write a config file

cat >config.yaml <<EOF
apiVersion: kubeadm.k8s.io/v1alpha1
kind: MasterConfiguration
api:
  advertiseAddress: 192.168.33.21
etcd:
  endpoints:
  - https://192.168.33.21:2379
  - https://192.168.33.22:2379
  - https://192.168.33.23:2379
  caFile: /etc/kubernetes/pki/etcd/ca.pem
  certFile: /etc/kubernetes/pki/etcd/client.pem
  keyFile: /etc/kubernetes/pki/etcd/client-key.pem
networking:
  podSubnet: 10.244.0.0/16
apiServerCertSANs:
- 192.168.33.100
apiServerExtraArgs:
  endpoint-reconciler-type: lease
token: mydemo.0123456789abcdef
EOF

yum install -y docker
systemctl enable docker && systemctl start docker

cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
EOF

setenforce 0

yum install -y kubelet kubeadm kubectl
systemctl enable kubelet && systemctl start kubelet

cat <<EOF >  /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
sysctl --system

swapoff -a

kubeadm init --config=config.yaml (SUCCESS)

systemctl enable kubelet.service
kubeadm init --config=config.yaml --ignore-preflight-checks=all

on master 2 and 3

scp root@<master0-ip-address>:/etc/kubernetes/pki/* /etc/kubernetes/pki


ON master1 

wget https://raw.githubusercontent.com/projectcalico/canal/master/k8s-install/1.7/rbac.yaml
wget https://raw.githubusercontent.com/projectcalico/canal/master/k8s-install/1.7/canal.yaml

Edit canal.yaml change canal_iface to eth1

kubectl apply -f rbac.yaml

ADDING WORKERS

yum install -y docker
systemctl enable docker && systemctl start docker

cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
EOF

setenforce 0

yum install -y kubelet kubeadm kubectl
systemctl enable kubelet && systemctl start kubelet

cat <<EOF >  /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
sysctl --system

swapoff -a

kubeadm join --token mydemo.0123456789abcdef 192.168.33.100:6443 --discovery-token-unsafe-skip-ca-verification

SWAP needs to be completely disabled or it will prevent kubelet starting after reboot.